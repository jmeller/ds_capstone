---
title: "Week 2: Milestone Report"
subtitle: "Data Science Capstone Project"
author: Jan
output: 
  html_notebook: 
    number_sections: yes
---

```{r load_dependencies, message=FALSE, warning=FALSE, echo=False, results='hide'}
library(tm)
library(dplyr)
library(parallel)
library(SnowballC)
library(ngram)
library(tidyverse)
```

# Executive summary

This report is part of the Coursera data science capstone project. I examine a subsample of the Swiftkey data set where I concentrate on the English texts. I sampled \approx 10\% of the total texts that where given in the form of blog entries, tweets or news articles. After generic preprocessing steps, a document-term matrix is generated. Also, I examined the combinations of single words and illustrated them by a first three-gram model.

# Load data

Here I sampled **100,000** rows from each of the three data sources that are US blogs, US tweets and US news articles.

```{r data_loading, message=FALSE, warning=FALSE, results='hide'}
# path to content
fnames <- list.files("Coursera-SwiftKey/final/en_US", full.names = TRUE)

fn_readfile <- function(fname, sampling=FALSE, n=1000){
  con <- file(fname, "r") 
  
  result_list <- list()
  
  while(length(result_list) < n | !sampling) {
    
    if (sampling){
      if(rbinom(1, 1, 0.5)){ # sample whether or not to consider this line for the text sample
        nextline <-  readLines(con, 1, encoding="UTF-8", warn=F)
        if(length(nextline) > 0){
          result_list[length(result_list) + 1] <- nextline
        } else {
          break
        }
      }
    } else {
      nextline <-  readLines(con, 1, encoding="UTF-8", warn=F)
      if(length(nextline) > 0){
        result_list[length(result_list) + 1] <- nextline
      } else {
        break
      }
    }
  }
  
  close(con) # close connection
  return(result_list)
}

# subsampling
us_texts_subsample <- lapply(fnames, fn_readfile, sampling=T, n=100000) %>% 
  Reduce(f=append, x=.) %>% 
  unlist %>%
  VectorSource %>%
  SimpleCorpus
```
```{r loading_full_texts, eval=FALSE, include=FALSE}
# full texts
us_texts_vector <- lapply(fnames, fn_readfile, sampling=FALSE) %>% Reduce(f=append, x=.) %>% unlist
us_texts <- SimpleCorpus(VectorSource(us_texts_vector))
```

# Processing

Several preprocessing steps were performed. I chose to first transform all characters to lower case, then remove white spaces between words, numbers and punctuation as well as english stopwords.

## Generic preprocessing steps
```{r preprocessing, results='hide', message=FALSE, warning=FALSE, }
# define stop words
word_removal <- function(x) removeWords(x, stopwords("english"))

preprocess_fns <- list(
  stripWhitespace,
  removeNumbers,
  removePunctuation,
  word_removal
)

us_preprocessed_subsample <- tm_map(us_texts_subsample, content_transformer(tolower)) %>%
  tm_map(FUN = tm_reduce, tmFuns = preprocess_fns)
```
```{r processing_full_texts, eval=FALSE, include=FALSE}
us_preprocessed <- tm_map(us_texts, content_transformer(tolower)) %>%
  tm_map(FUN = tm_reduce, tmFuns = preprocess_fns)
```

## Document-term Matrix

In the next step, I generate the document-term matrix for words that are of length between 4 and 20 characters. 
```{r}
dtm <- DocumentTermMatrix(us_preprocessed_subsample, control=list(wordLengths=c(4, 20)))
```
```{r filter_for_sparsity, echo=FALSE}
sparsity_threshold <- 0.999
freq <- colSums(as.matrix(removeSparseTerms(dtm, sparsity_threshold)))
ord <- order(freq, decreasing=TRUE)
```

In total,there are ```r length(freq)``` items that are less sparse than ```r sparsity_threshold```, i.e., all of these words occur in at least ```r (1-sparsity_threshold)*100```% of the documents. The following graph visualizes the number of occurences of terms that occur more than 5000 times in the corpus.
```{r plot_data, echo=FALSE}
plot_data <- data.frame(term=names(freq),occurrences=freq)
plot <- data.frame(term=names(freq),occurrences=freq) %>% filter(occurrences > 5000) %>% 
  ggplot(aes(x=reorder(term, -occurrences), y=occurrences)) + geom_bar(stat='identity') + theme_bw() + theme(axis.text.x=element_text(angle = 90, vjust = 0.5)) + labs(x='term', y='# of occurrences')
plot
```

In the following is a table of the 10 least frequently occurring terms in the corpus:

```{r, echo=FALSE}
freq[tail(ord, n = 10)]
```



# Analysis of N-grams

I used the ngram #rstats-package due to its good performance for large data sets. 

```{r}
n <- 3
strings <- concatenate(lapply(us_preprocessed_subsample, "[", 1))
ng <- ngram(strings, n=n)
```


```{r, echo=FALSE, results='hide'}
print(ng, output ="truncated")
```
```{r echo=FALSE}
get.phrasetable(ng)
```

# Next steps

In the upcoming weeks I plan on using the ngram package to further develop my prediction model. This will then be integrated into a shiny app which makes next word predictions on demand. For this reason, I will further focus only on the English language corpus. 